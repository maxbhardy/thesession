{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096de4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import pathlib\n",
    "\n",
    "from thesession.converter import ABCMusicConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABCMusicConverter(\"cooleys.abc\", \"cooleys\").to_mp3(\n",
    "    instrument=\"flute\",\n",
    "    tempo=190,\n",
    "    max_notes=300,\n",
    "    cut_silence=30,\n",
    "    start=0.57,\n",
    "    noise_amplitude=0.001,\n",
    "    duration=120,\n",
    "    vbr=8,\n",
    "    clean_files=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe4d10",
   "metadata": {},
   "source": [
    "# CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d679915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.weight \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from laion_clap import CLAP_Module\n",
    "\n",
    "# Load pretrained CLAP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLAP_Module(enable_fusion=True) # For using fusion to manage >10s clips\n",
    "model.load_ckpt()  # downloads pretrained weights\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f64f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sr = torchaudio.load(\"cooleys.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_audio(filepath, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
    "    return waveform.mean(dim=0).unsqueeze(0)  # Convert to mono, add batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "724627b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor0 = load_audio(\"audio/1_cooleys/36371_0.mp3\")\n",
    "audio_tensor1 = load_audio(\"audio/1_cooleys/36371_1.mp3\")\n",
    "audio_tensor2 = load_audio(\"audio/8_the_banshee/32846_0.mp3\")\n",
    "audio_tensor3 = load_audio(\"audio/8_the_banshee/32846_1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6944d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embedding0 = model.get_audio_embedding_from_data(audio_tensor0, use_tensor=True)\n",
    "    embedding1 = model.get_audio_embedding_from_data(audio_tensor1, use_tensor=True)\n",
    "    embedding2 = model.get_audio_embedding_from_data(audio_tensor2, use_tensor=True)\n",
    "    embedding3 = model.get_audio_embedding_from_data(audio_tensor3, use_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b25152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.cat([embedding0, embedding1, embedding2, embedding3], dim=0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25438c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8502, 0.8558, 0.8473],\n",
       "        [0.8502, 1.0000, 0.9414, 0.8412],\n",
       "        [0.8558, 0.9414, 1.0000, 0.8524],\n",
       "        [0.8473, 0.8412, 0.8524, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.cosine_similarity(embeddings[None, :, :], embeddings[:, None, :], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692eb79",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc47584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from thesession.dataset import TheSessionDataset\n",
    "\n",
    "path = pathlib.Path(\"audio_flac\")\n",
    "tunes = [p.stem for p in path.iterdir() if p.is_dir()]\n",
    "tunes.sort()\n",
    "\n",
    "tunes_df = pd.DataFrame({\"tune\": tunes.copy(), \"dataset\": None})\n",
    "\n",
    "prng = np.random.default_rng(42)\n",
    "tunes = prng.permutation(tunes)\n",
    "\n",
    "tunes_df.loc[tunes_df[\"tune\"].isin(tunes[0 : int(0.15 * len(tunes))]), \"dataset\"] = \"test\"\n",
    "tunes_df.loc[tunes_df[\"tune\"].isin(tunes[int(0.15 * len(tunes)) : int(0.3 * len(tunes))]), \"dataset\"] = \"validation\"\n",
    "tunes_df.loc[tunes_df[\"tune\"].isin(tunes[int(0.3 * len(tunes)) :]), \"dataset\"] = \"train\"\n",
    "\n",
    "\n",
    "tunes_df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0c08af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 26344\n",
      "Validation data 5534\n",
      "Test data 5728\n"
     ]
    }
   ],
   "source": [
    "test_subset = tunes_df.loc[tunes_df[\"dataset\"] == \"test\", \"tune\"]\n",
    "val_subset = tunes_df.loc[tunes_df[\"dataset\"] == \"validation\", \"tune\"]\n",
    "train_subset = tunes_df.loc[tunes_df[\"dataset\"] == \"train\", \"tune\"]\n",
    "\n",
    "train_dataset = TheSessionDataset(\"audio_flac\", subset=train_subset, sampling_rate=48000, fmt=\".flac\")\n",
    "val_dataset = TheSessionDataset(\"audio_flac\", subset=val_subset, sampling_rate=48000, fmt=\".flac\")\n",
    "test_dataset = TheSessionDataset(\"audio_flac\", subset=test_subset, sampling_rate=48000, fmt=\".flac\")\n",
    "\n",
    "print(\"Training data\", len(train_dataset))\n",
    "print(\"Validation data\", len(val_dataset))\n",
    "print(\"Test data\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f93cdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0138, -0.0157, -0.0151,  ..., -0.0132, -0.0090, -0.0040]),\n",
       " tensor([ 0.0086,  0.0108,  0.0123,  ..., -0.0052, -0.0035, -0.0016]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87a21b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0609)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[32][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e45905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0386)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[32][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acb6aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 ms ± 3.97 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2cefd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a35a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration:  8.030320544996357\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i, (x1, x2) in enumerate(train_loader):\n",
    "    if i < 20:\n",
    "        print(i, end=\"\\r\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"Duration: \", end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b35ab",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476dfbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.weight \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "from thesession.model import TheSessionModel\n",
    "\n",
    "model = TheSessionModel()\n",
    "model.load_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62a1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling gradient for parameter clap_model.model.logit_scale_a\n",
      "Disabling gradient for parameter clap_model.model.logit_scale_t\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.spectrogram_extractor.stft.conv_real.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.spectrogram_extractor.stft.conv_imag.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.logmel_extractor.melW\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.bn0.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.bn0.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.norm.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.norm.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.mel_conv2d.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.mel_conv2d.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.0.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.0.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.3.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.3.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.4.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.local_att.4.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.4.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.4.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.5.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.patch_embed.fusion_model.global_att.5.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.0.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.blocks.1.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.downsample.reduction.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.downsample.norm.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.0.downsample.norm.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.0.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.blocks.1.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.downsample.reduction.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.downsample.norm.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.1.downsample.norm.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.0.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.1.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.2.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.3.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.4.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.blocks.5.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.downsample.reduction.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.downsample.norm.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.2.downsample.norm.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.0.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.norm1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.norm1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.attn.qkv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.attn.qkv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.attn.proj.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.attn.proj.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.norm2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.norm2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.mlp.fc1.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.mlp.fc1.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.mlp.fc2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.layers.3.blocks.1.mlp.fc2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.norm.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.norm.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.tscam_conv.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.tscam_conv.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.head.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_branch.head.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.embeddings.word_embeddings.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.embeddings.position_embeddings.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.embeddings.token_type_embeddings.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.embeddings.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.embeddings.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.0.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.1.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.2.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.3.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.4.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.5.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.6.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.7.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.8.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.9.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.10.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.query.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.query.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.key.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.key.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.value.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.self.value.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.intermediate.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.intermediate.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.output.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.output.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.output.LayerNorm.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.encoder.layer.11.output.LayerNorm.bias\n",
      "Disabling gradient for parameter clap_model.model.text_branch.pooler.dense.weight\n",
      "Disabling gradient for parameter clap_model.model.text_branch.pooler.dense.bias\n",
      "Disabling gradient for parameter clap_model.model.text_transform.sequential.0.weight\n",
      "Disabling gradient for parameter clap_model.model.text_transform.sequential.0.bias\n",
      "Disabling gradient for parameter clap_model.model.text_transform.sequential.3.weight\n",
      "Disabling gradient for parameter clap_model.model.text_transform.sequential.3.bias\n",
      "Disabling gradient for parameter clap_model.model.text_projection.0.weight\n",
      "Disabling gradient for parameter clap_model.model.text_projection.0.bias\n",
      "Disabling gradient for parameter clap_model.model.text_projection.2.weight\n",
      "Disabling gradient for parameter clap_model.model.text_projection.2.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_transform.sequential.0.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_transform.sequential.0.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_transform.sequential.3.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_transform.sequential.3.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_projection.0.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_projection.0.bias\n",
      "Disabling gradient for parameter clap_model.model.audio_projection.2.weight\n",
      "Disabling gradient for parameter clap_model.model.audio_projection.2.bias\n"
     ]
    }
   ],
   "source": [
    "model.toggle_gradients(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef99e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling gradient for parameter clap_model.model.audio_projection.0.weight\n",
      "Enabling gradient for parameter clap_model.model.audio_projection.0.bias\n",
      "Enabling gradient for parameter clap_model.model.audio_projection.2.weight\n",
      "Enabling gradient for parameter clap_model.model.audio_projection.2.bias\n"
     ]
    }
   ],
   "source": [
    "model.toggle_gradients(True, ['clap_model.model.audio_projection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a99a6d",
   "metadata": {},
   "source": [
    "# Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d23f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from thesession.dataset import TheSessionDataset\n",
    "from thesession.model import TheSessionModel\n",
    "from thesession.training import eval_model, NTXentLoss\n",
    "\n",
    "# Dataset\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "test_subset = dataset.loc[dataset[\"dataset\"] == \"test\", \"tune\"]\n",
    "\n",
    "test_dataset = TheSessionDataset(\n",
    "    \"audio_flac\", subset=test_subset, sampling_rate=48000, fmt=\".flac\"\n",
    ")\n",
    "\n",
    "model = TheSessionModel(device=\"cuda\")\n",
    "criterion = NTXentLoss(temperature=0.05)\n",
    "model.eval()\n",
    "\n",
    "with open(\"testing.csv\", \"w\") as f:\n",
    "    f.write(\"\\nstep,test_loss,test_accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a04f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch 1/179\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 79.15 seconds\n",
      "Test Loss: 4.1314 | Test Accuracy: 0.0402 | \n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nbaseline,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ee8306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.weight \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n",
      "Test batch 1/179\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 78.15 seconds\n",
      "Test Loss: 5.5868 | Test Accuracy: 0.0102 | \n"
     ]
    }
   ],
   "source": [
    "# Pretrained weights\n",
    "model.load_ckpt()\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\npretrain,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899b8439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 76.79 seconds\n",
      "Test Loss: 1.3755 | Test Accuracy: 0.8455 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step1_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep1,{loss},{acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8835e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 77.31 seconds\n",
      "Test Loss: 1.2046 | Test Accuracy: 0.8943 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step2_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep2,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18360f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 78.79 seconds\n",
      "Test Loss: 0.3282 | Test Accuracy: 0.9010 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step3_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep3,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e0c583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 78.08 seconds\n",
      "Test Loss: 0.1179 | Test Accuracy: 0.9696 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step4_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep4,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387a1c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 78.67 seconds\n",
      "Test Loss: 0.1029 | Test Accuracy: 0.9763 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step5_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep5,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e822038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of evaluation on test data: 76.33 seconds\n",
      "Test Loss: 0.0330 | Test Accuracy: 0.9923 | \n"
     ]
    }
   ],
   "source": [
    "model.load(\"models/step6_best.pt\")\n",
    "loss, acc = eval_model(model, test_dataset, criterion, \"cuda\", num_workers=6)\n",
    "\n",
    "with open(\"testing.csv\", \"a\") as f:\n",
    "    f.write(f\"\\nstep6,{loss},{acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4bc87",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309ebfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from thesession.retriever import TheSessionRetriever, get_database_url\n",
    "\n",
    "dotenv.load_dotenv(\"thesession-db/.env\")\n",
    "\n",
    "url = get_database_url(\n",
    "    os.getenv(\"POSTGRES_USER\"),\n",
    "    os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "    port=15432,\n",
    "    database=os.getenv(\"POSTGRES_DB\")\n",
    ")\n",
    "\n",
    "retriever = TheSessionRetriever(url, \"models/step6_best.pt\", backend=\"soundfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0dae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TuneID</th>\n",
       "      <th>TuneVersionID</th>\n",
       "      <th>TuneTitle</th>\n",
       "      <th>TuneAuthor</th>\n",
       "      <th>TuneURL</th>\n",
       "      <th>TuneType</th>\n",
       "      <th>Tunebooks</th>\n",
       "      <th>TuneVersion</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>36384</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 14\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.940252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>36379</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 9\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...</td>\n",
       "      <td>0.938674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36371</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 1\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...</td>\n",
       "      <td>0.926875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>36383</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 13\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.925647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>36389</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 19\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.914596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>36373</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 3\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...</td>\n",
       "      <td>0.909853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>36395</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 25\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.893879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>36386</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 16\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.892249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>36392</td>\n",
       "      <td>Cooley’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/1</td>\n",
       "      <td>reel</td>\n",
       "      <td>6090</td>\n",
       "      <td>X: 22\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...</td>\n",
       "      <td>0.890966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17758</td>\n",
       "      <td>36231</td>\n",
       "      <td>Fiddler Leahy’s</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/17758</td>\n",
       "      <td>reel</td>\n",
       "      <td>1</td>\n",
       "      <td>X: 1\\nT: Fiddler Leahy's\\nR: reel\\nM: 4/4\\nL: ...</td>\n",
       "      <td>0.882060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TuneID  TuneVersionID        TuneTitle TuneAuthor  \\\n",
       "0       1          36384         Cooley’s       None   \n",
       "1       1          36379         Cooley’s       None   \n",
       "2       1          36371         Cooley’s       None   \n",
       "3       1          36383         Cooley’s       None   \n",
       "4       1          36389         Cooley’s       None   \n",
       "5       1          36373         Cooley’s       None   \n",
       "6       1          36395         Cooley’s       None   \n",
       "7       1          36386         Cooley’s       None   \n",
       "8       1          36392         Cooley’s       None   \n",
       "9   17758          36231  Fiddler Leahy’s       None   \n",
       "\n",
       "                              TuneURL TuneType  Tunebooks  \\\n",
       "0      https://thesession.org/tunes/1     reel       6090   \n",
       "1      https://thesession.org/tunes/1     reel       6090   \n",
       "2      https://thesession.org/tunes/1     reel       6090   \n",
       "3      https://thesession.org/tunes/1     reel       6090   \n",
       "4      https://thesession.org/tunes/1     reel       6090   \n",
       "5      https://thesession.org/tunes/1     reel       6090   \n",
       "6      https://thesession.org/tunes/1     reel       6090   \n",
       "7      https://thesession.org/tunes/1     reel       6090   \n",
       "8      https://thesession.org/tunes/1     reel       6090   \n",
       "9  https://thesession.org/tunes/17758     reel          1   \n",
       "\n",
       "                                         TuneVersion  Similarity  \n",
       "0  X: 14\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.940252  \n",
       "1  X: 9\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...    0.938674  \n",
       "2  X: 1\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...    0.926875  \n",
       "3  X: 13\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.925647  \n",
       "4  X: 19\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.914596  \n",
       "5  X: 3\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK:...    0.909853  \n",
       "6  X: 25\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.893879  \n",
       "7  X: 16\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.892249  \n",
       "8  X: 22\\nT: Cooley's\\nR: reel\\nM: 4/4\\nL: 1/8\\nK...    0.890966  \n",
       "9  X: 1\\nT: Fiddler Leahy's\\nR: reel\\nM: 4/4\\nL: ...    0.882060  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"audio_flac/1_cooleys/36371_0.flac\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f02d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TuneID</th>\n",
       "      <th>TuneVersionID</th>\n",
       "      <th>TuneTitle</th>\n",
       "      <th>TuneAuthor</th>\n",
       "      <th>TuneURL</th>\n",
       "      <th>TuneType</th>\n",
       "      <th>Tunebooks</th>\n",
       "      <th>TuneVersion</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>32850</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 5\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...</td>\n",
       "      <td>0.933319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>32864</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 19\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8...</td>\n",
       "      <td>0.893997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>32858</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 13\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8...</td>\n",
       "      <td>0.869655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>32849</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 4\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...</td>\n",
       "      <td>0.861172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24360</td>\n",
       "      <td>46018</td>\n",
       "      <td>The Rabbit And The Lamb</td>\n",
       "      <td>Crisdean MacDonald</td>\n",
       "      <td>https://thesession.org/tunes/24360</td>\n",
       "      <td>jig</td>\n",
       "      <td>7</td>\n",
       "      <td>X: 2\\nT: The Rabbit And The Lamb\\nR: jig\\nM: 6...</td>\n",
       "      <td>0.840943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>32846</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 1\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...</td>\n",
       "      <td>0.822947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17942</td>\n",
       "      <td>55451</td>\n",
       "      <td>John Joe Moroney’s Favourite</td>\n",
       "      <td>Dermot Lernihan</td>\n",
       "      <td>https://thesession.org/tunes/17942</td>\n",
       "      <td>reel</td>\n",
       "      <td>6</td>\n",
       "      <td>X: 1\\nT: John Joe Moroney's Favourite\\nR: reel...</td>\n",
       "      <td>0.822024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>32847</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 2\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...</td>\n",
       "      <td>0.821859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>32854</td>\n",
       "      <td>The Banshee</td>\n",
       "      <td>James McMahon</td>\n",
       "      <td>https://thesession.org/tunes/8</td>\n",
       "      <td>reel</td>\n",
       "      <td>4120</td>\n",
       "      <td>X: 9\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...</td>\n",
       "      <td>0.817459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3484</td>\n",
       "      <td>34596</td>\n",
       "      <td>Adele’s Bath</td>\n",
       "      <td>Jacob Fournel</td>\n",
       "      <td>https://thesession.org/tunes/3484</td>\n",
       "      <td>jig</td>\n",
       "      <td>16</td>\n",
       "      <td>X: 1\\nT: Adele's Bath\\nR: jig\\nM: 6/8\\nL: 1/8\\...</td>\n",
       "      <td>0.812761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TuneID  TuneVersionID                     TuneTitle          TuneAuthor  \\\n",
       "0       8          32850                   The Banshee       James McMahon   \n",
       "1       8          32864                   The Banshee       James McMahon   \n",
       "2       8          32858                   The Banshee       James McMahon   \n",
       "3       8          32849                   The Banshee       James McMahon   \n",
       "4   24360          46018       The Rabbit And The Lamb  Crisdean MacDonald   \n",
       "5       8          32846                   The Banshee       James McMahon   \n",
       "6   17942          55451  John Joe Moroney’s Favourite     Dermot Lernihan   \n",
       "7       8          32847                   The Banshee       James McMahon   \n",
       "8       8          32854                   The Banshee       James McMahon   \n",
       "9    3484          34596                  Adele’s Bath       Jacob Fournel   \n",
       "\n",
       "                              TuneURL TuneType  Tunebooks  \\\n",
       "0      https://thesession.org/tunes/8     reel       4120   \n",
       "1      https://thesession.org/tunes/8     reel       4120   \n",
       "2      https://thesession.org/tunes/8     reel       4120   \n",
       "3      https://thesession.org/tunes/8     reel       4120   \n",
       "4  https://thesession.org/tunes/24360      jig          7   \n",
       "5      https://thesession.org/tunes/8     reel       4120   \n",
       "6  https://thesession.org/tunes/17942     reel          6   \n",
       "7      https://thesession.org/tunes/8     reel       4120   \n",
       "8      https://thesession.org/tunes/8     reel       4120   \n",
       "9   https://thesession.org/tunes/3484      jig         16   \n",
       "\n",
       "                                         TuneVersion  Similarity  \n",
       "0  X: 5\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...    0.933319  \n",
       "1  X: 19\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8...    0.893997  \n",
       "2  X: 13\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8...    0.869655  \n",
       "3  X: 4\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...    0.861172  \n",
       "4  X: 2\\nT: The Rabbit And The Lamb\\nR: jig\\nM: 6...    0.840943  \n",
       "5  X: 1\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...    0.822947  \n",
       "6  X: 1\\nT: John Joe Moroney's Favourite\\nR: reel...    0.822024  \n",
       "7  X: 2\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...    0.821859  \n",
       "8  X: 9\\nT: The Banshee\\nR: reel\\nM: 4/4\\nL: 1/8\\...    0.817459  \n",
       "9  X: 1\\nT: Adele's Bath\\nR: jig\\nM: 6/8\\nL: 1/8\\...    0.812761  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"audio_flac/8_the_banshee/32850_1.flac\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6a9764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/session/lib64/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TuneID</th>\n",
       "      <th>TuneVersionID</th>\n",
       "      <th>TuneTitle</th>\n",
       "      <th>TuneAuthor</th>\n",
       "      <th>TuneURL</th>\n",
       "      <th>TuneType</th>\n",
       "      <th>Tunebooks</th>\n",
       "      <th>TuneVersion</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7346</td>\n",
       "      <td>3724</td>\n",
       "      <td>Hommage à Edmond Parizeau</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7346</td>\n",
       "      <td>reel</td>\n",
       "      <td>114</td>\n",
       "      <td>X: 4\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...</td>\n",
       "      <td>0.918252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7346</td>\n",
       "      <td>3725</td>\n",
       "      <td>Hommage à Edmond Parizeau</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7346</td>\n",
       "      <td>reel</td>\n",
       "      <td>114</td>\n",
       "      <td>X: 5\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...</td>\n",
       "      <td>0.899184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7346</td>\n",
       "      <td>3721</td>\n",
       "      <td>Hommage à Edmond Parizeau</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7346</td>\n",
       "      <td>reel</td>\n",
       "      <td>114</td>\n",
       "      <td>X: 1\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...</td>\n",
       "      <td>0.895876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7346</td>\n",
       "      <td>3723</td>\n",
       "      <td>Hommage à Edmond Parizeau</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7346</td>\n",
       "      <td>reel</td>\n",
       "      <td>114</td>\n",
       "      <td>X: 3\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...</td>\n",
       "      <td>0.856666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7346</td>\n",
       "      <td>3722</td>\n",
       "      <td>Hommage à Edmond Parizeau</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7346</td>\n",
       "      <td>reel</td>\n",
       "      <td>114</td>\n",
       "      <td>X: 2\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...</td>\n",
       "      <td>0.825461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7937</td>\n",
       "      <td>13808</td>\n",
       "      <td>Jota Da Maia</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7937</td>\n",
       "      <td>waltz</td>\n",
       "      <td>23</td>\n",
       "      <td>X: 2\\nT: Jota Da Maia\\nR: waltz\\nM: 3/4\\nL: 1/...</td>\n",
       "      <td>0.818952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7937</td>\n",
       "      <td>13807</td>\n",
       "      <td>Jota Da Maia</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7937</td>\n",
       "      <td>waltz</td>\n",
       "      <td>23</td>\n",
       "      <td>X: 1\\nT: Jota Da Maia\\nR: waltz\\nM: 3/4\\nL: 1/...</td>\n",
       "      <td>0.818717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14494</td>\n",
       "      <td>46477</td>\n",
       "      <td>L’Antre</td>\n",
       "      <td>Fred Guichen</td>\n",
       "      <td>https://thesession.org/tunes/14494</td>\n",
       "      <td>waltz</td>\n",
       "      <td>14</td>\n",
       "      <td>X: 2\\nT: L'Antre\\nR: waltz\\nM: 3/4\\nL: 1/8\\nK:...</td>\n",
       "      <td>0.799746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20083</td>\n",
       "      <td>43476</td>\n",
       "      <td>Bridal Festival Quadrille</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/20083</td>\n",
       "      <td>jig</td>\n",
       "      <td>8</td>\n",
       "      <td>X: 2\\nT: Bridal Festival Quadrille\\nR: jig\\nM:...</td>\n",
       "      <td>0.786909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7116</td>\n",
       "      <td>50095</td>\n",
       "      <td>Cuckold Come Out Of The Amrey</td>\n",
       "      <td>None</td>\n",
       "      <td>https://thesession.org/tunes/7116</td>\n",
       "      <td>polka</td>\n",
       "      <td>82</td>\n",
       "      <td>X: 2\\nT: Cuckold Come Out Of The Amrey\\nR: pol...</td>\n",
       "      <td>0.779676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TuneID  TuneVersionID                      TuneTitle    TuneAuthor  \\\n",
       "0    7346           3724      Hommage à Edmond Parizeau          None   \n",
       "1    7346           3725      Hommage à Edmond Parizeau          None   \n",
       "2    7346           3721      Hommage à Edmond Parizeau          None   \n",
       "3    7346           3723      Hommage à Edmond Parizeau          None   \n",
       "4    7346           3722      Hommage à Edmond Parizeau          None   \n",
       "5    7937          13808                   Jota Da Maia          None   \n",
       "6    7937          13807                   Jota Da Maia          None   \n",
       "7   14494          46477                        L’Antre  Fred Guichen   \n",
       "8   20083          43476      Bridal Festival Quadrille          None   \n",
       "9    7116          50095  Cuckold Come Out Of The Amrey          None   \n",
       "\n",
       "                              TuneURL TuneType  Tunebooks  \\\n",
       "0   https://thesession.org/tunes/7346     reel        114   \n",
       "1   https://thesession.org/tunes/7346     reel        114   \n",
       "2   https://thesession.org/tunes/7346     reel        114   \n",
       "3   https://thesession.org/tunes/7346     reel        114   \n",
       "4   https://thesession.org/tunes/7346     reel        114   \n",
       "5   https://thesession.org/tunes/7937    waltz         23   \n",
       "6   https://thesession.org/tunes/7937    waltz         23   \n",
       "7  https://thesession.org/tunes/14494    waltz         14   \n",
       "8  https://thesession.org/tunes/20083      jig          8   \n",
       "9   https://thesession.org/tunes/7116    polka         82   \n",
       "\n",
       "                                         TuneVersion  Similarity  \n",
       "0  X: 4\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...    0.918252  \n",
       "1  X: 5\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...    0.899184  \n",
       "2  X: 1\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...    0.895876  \n",
       "3  X: 3\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...    0.856666  \n",
       "4  X: 2\\nT: Hommage à Edmond Parizeau\\nR: reel\\nM...    0.825461  \n",
       "5  X: 2\\nT: Jota Da Maia\\nR: waltz\\nM: 3/4\\nL: 1/...    0.818952  \n",
       "6  X: 1\\nT: Jota Da Maia\\nR: waltz\\nM: 3/4\\nL: 1/...    0.818717  \n",
       "7  X: 2\\nT: L'Antre\\nR: waltz\\nM: 3/4\\nL: 1/8\\nK:...    0.799746  \n",
       "8  X: 2\\nT: Bridal Festival Quadrille\\nR: jig\\nM:...    0.786909  \n",
       "9  X: 2\\nT: Cuckold Come Out Of The Amrey\\nR: pol...    0.779676  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"audio_flac/7346_hommage__edmond_parizeau/3724_4.flac\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f91aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
